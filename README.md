# CALIBER
### *Making AI Greener, One Query at a Time üå±*

**Authors:** Stefano Cirillo, Giuseppe Polese, Giandomenico Solimando, Nicola Zannone

![System Architecture](img/Architecture.png)

## üìÑ Abstract

The adoption of Large Language Models (LLMs) for translating natural language into SQL (NL2SQL) queries is becoming a standard practice for interacting with databases. While recent studies have focused almost exclusively on query correctness, the efficiency and carbon footprint of LLM-generated SQL have received little attention. 
Semantically correct queries can still be highly inefficient, resulting in significantly higher execution times and energy consumption in real DBMSs.
To this end, in this paper, we present CALIBER, an interactive framework for benchmarking the environmental impact of NL2SQL pipelines. 
CALIBER allows users to submit natural language queries, generate SQL queries with LLMs, and execute them directly on multiple databases.
Through four types of benchmarking, it measures both inference and execution energy of generated queries using hardware sensors, TDP estimates, and region-specific carbon intensity factors. The tool also includes automatic prompt-engineering techniques to drive the generation of greener queries.
In this demo, we show how CALIBER enables direct comparison of LLM energy efficiency, sustainability trade-offs, and supports environmentally informed model selection for NL2SQL applications.

---

## üåü Key Features

CALIBER is an advanced web application designed to bridge the gap between natural language questions and database queries, with a strong focus on sustainability and performance.

### üåç Eco-Friendly AI
Monitor and reduce CO‚ÇÇ emissions generated by your SQL queries and AI operations. The application provides real-time tracking of energy consumption (CPU/GPU) and carbon footprint estimation.

### ‚ö° Query Optimization ("Greenefy")
Automatically optimize generated SQL queries. The system doesn't just write SQL; it attempts to rewrite queries to be more efficient and environmentally friendly, comparing the execution cost of different variations.

### üìä Advanced Analytics & Profiling
In-depth dataset analysis including:
*   **Detailed Statistics**: Row/column counts, missing values, memory usage.
*   **Column Analysis**: Data types, cardinality, and distribution.
*   **PII Detection**: Uses **spaCy** to identify sensitive information (names, emails, etc.).
*   **Relational Profiling**: Primary key suggestions and anomaly detection.

### üß† Multi-LLM Support
Flexible "adapter" architecture supporting various backends:
*   **ü§ó Hugging Face**: Download and use models directly from the Hub.
*   **ü¶ô Ollama**: Integrate with local Ollama instances.
*   **üß™ LM Studio**: Connect to local LM Studio servers.
*   **üì§ Local Models**: Load GGUF/Transformer models manually.

### üéØ Benchmarking
A dedicated suite to compare performance, quality, and environmental impact across different models and hardware configurations.

### üß¨ Synthetic Data
Generate realistic synthetic datasets for testing and development, ensuring privacy while maintaining statistical properties.

---

## üîß Project Architecture

The codebase is organized into modular components to ensure scalability and maintainability:

*   **`app.py`**: The main entry point of the Streamlit application, handling session state and navigation.
*   **`GUI/`**: Contains the user interface modules. Each file represents a specific tab or page (e.g., `load_db_tab.py`, `relational_profiling_tab.py`, `benchmarking_tab.py`).
*   **`llm_adapters/`**: Interfaces for different LLM backends (`huggingface_adapter.py`, `ollama_adapter.py`, etc.), providing a unified API for generation.
*   **`db_adapters/`**: Manages database connections and operations. `DBManager.py` handles the abstraction for different DBMS (SQLite, MySQL, DuckDB).
*   **`utils/`**: Helper functions for prompt engineering (`prompt_builder.py`), SQL cleaning (`query_cleaner.py`), and system monitoring (`system_monitor_utilities.py`).
*   **`setup/`**: Scripts and configuration files for installation and environment setup.
*   **`img/`**: Stores static assets like the architecture diagram.

---

## üöÄ Getting Started

### Prerequisites
*   **Python 3.9+**
*   **Git**
*   **Ollama** (optional, for local inference)
*   **LM Studio** (optional, for local inference)

### Installation

You can set up the environment using the provided scripts or manually.

#### Option 1: Automated Setup (Windows)
Run the included batch script to set up a Conda environment or Python venv automatically:
```bat
setup\install.bat
```
Follow the on-screen instructions to choose your environment type.

#### Option 2: Manual Setup (Conda)
```bash
conda env create -f setup/conda_env.yml
conda activate Demo_EDBT
```

#### Option 3: Manual Setup (Pip)
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

pip install -r setup/requirements.txt
```

### Model Setup

#### 1. Download spaCy Models
Required for text analysis and PII detection:
```bash
python -m spacy download en_core_web_sm  # English model
python -m spacy download it_core_news_sm  # Italian model
```

#### 2. Download LLM (Hugging Face)
To download the default model (`meta-llama/Meta-Llama-3-8B`) from Hugging Face:
```bash
python setup/download_model
```
*Note: You may need to set your `HF_TOKEN` environment variable or edit the script if the model requires authentication.*

#### 3. Local Servers (Ollama / LM Studio)
*   **Ollama**: Ensure Ollama is installed and running (`ollama serve`). Pull a model like `llama3`.
*   **LM Studio**: Start the local server feature in LM Studio to expose the API.

---

## üìã Usage Guide

### 1. Welcome & Dashboard
Upon launching (`streamlit run app.py`), you are greeted by the dashboard summarizing the project's core pillars: Eco-Friendly AI, Query Optimization, and Analytics.

### 2. üìÑ Load Dataset
*   **Source**: Choose between **Local Files** (CSV, Parquet, etc.) or **DBMS Connection** (MySQL, SQLite).
*   **Profiling**: Once loaded, the app automatically profiles the data, showing statistics, data types, and potential issues.

### 3. ü§ñ Load Model
*   Navigate to the **Load Model** tab.
*   Select your provider:
    *   **Online Model**: Search and download from Hugging Face.
    *   **Local Model**: Connect to **Ollama** or **LM Studio**, or load a local file.
*   Click **"Set as active LLM"** to confirm.

### 4. üß™ Generate & Evaluate Query
*   Enter a natural language question (e.g., *"Show me the top 5 customers by spending"*).
*   Click **üöÄ Generate**.
*   **Results**:
    *   **SQL Query**: The generated SQL.
    *   **Execution**: The query result set.
    *   **Green Metrics**: Charts showing CPU/GPU usage and CO‚ÇÇ emissions for the generation process.
    *   **Alternatives**: The system may propose optimized versions of the query.

### 5. üéØ Benchmarking
Use this tab to run systematic tests against your dataset to evaluate different models or prompts, recording accuracy and resource consumption.

### 6. üß¨ Synthetic Data
Generate new datasets based on the statistical properties of your loaded data. Useful for creating privacy-safe test environments.


## Video Demo
The video demonstration can be accessed via the following link: https://www.youtube.com/watch?v=WqILg1SoQ2g
